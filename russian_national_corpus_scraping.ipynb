{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian National Corpus scraping\n",
    "## extract keywords in context (KWIC) data and metada from rnc\n",
    "by Jeremy Fahringer (@jfahringer)\n",
    "\n",
    "Nov.  2019\n",
    "\n",
    "## Disclaimer\n",
    " Use of this scraper is subject to the terms and conditions of the Russian National Corpus available at http://ruscorpora.ru/new/corpora-usage.html . Data extracted must abide by those terms.\n",
    "\n",
    "## initial plan\n",
    "\n",
    "1. create list of words to query (currently paired)\n",
    "2. query rnc url for each word [example for конфликт]( http://processing.ruscorpora.ru/search.xml?sort=i_grtagging&lang=ru&lex1=%20%D0%BA%D0%BE%D0%BD%D1%84%D0%BB%D0%B8%D0%BA%D1%82&startyear=1984&text=lexgramm&max1=1&sem-mod2=sem&sem-mod2=sem2&gramm1=S,(nom%7Cvoc%7Cgen%7Cgen2%7Cdat%7Cacc%7Cacc2%7Cins%7Cloc%7Cloc2%7Cadnum)&sem-mod1=sem&sem-mod1=sem2&level1=0&level2=0&endyear=2004&parent2=0&parent1=0&min1=1&out=kwic&nodia=1&mode=main&p=1500 )\n",
    "\n",
    "```\n",
    "http://processing.ruscorpora.ru/search.xml?sort=i_grtagging&lang=ru&lex1=%20%D0%BA%D0%BE%D0%BD%D1%84%D0%BB%D0%B8%D0%BA%D1%82&startyear=1984&text=lexgramm&max1=1&sem-mod2=sem&sem-mod2=sem2&gramm1=S,(nom%7Cvoc%7Cgen%7Cgen2%7Cdat%7Cacc%7Cacc2%7Cins%7Cloc%7Cloc2%7Cadnum)&sem-mod1=sem&sem-mod1=sem2&level1=0&level2=0&endyear=2004&parent2=0&parent1=0&min1=1&out=kwic&nodia=1&mode=main&p=1500\n",
    "```\n",
    "\n",
    "3. replace GET parameter lex1 with russian word to get KWIC search results (10 per page).... for all pages (ex. 11521 results = 1152 pages, lex1= конфликт )\n",
    "\n",
    "4. save original html data\n",
    "5. use beautiful soup to scrape 3 columns (context of 5 on left, KW, 5 on right) plus get date from popover alt text (in msg tag)\n",
    "6. append to output tsv file\n",
    "7. repeat for each page of results for that word\n",
    "8. save output tsv file for that word\n",
    "9. repeat for next word in list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in modules:\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import csv\n",
    "import time\n",
    "import argparse\n",
    "import os.path\n",
    "import itertools\n",
    "\n",
    "# Third-party modules provided by Anaconda:\n",
    "import requests\n",
    "import bs4  # also requires parser 'lxml' to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNC query word list\n",
    "# use a nested array to include set ordering, otherwise use a single array of strings\n",
    "word_pairs = [\n",
    "                [\"тест\", \"речь\"],\n",
    "                [\"еда\", \"питание\"],\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian National Corpus query parameters\n",
    "rnc_query_url = \"http://processing.ruscorpora.ru/search.xml?sort=i_grtagging&lang=ru&lex1=\"\n",
    "rnc_query_lex1 = \"конфликт\"\n",
    "rnc_query_params = \"&startyear=1984&text=lexgramm&max1=1&sem-mod2=sem&sem-mod2=sem2&gramm1=S,(nom%7Cvoc%7Cgen%7Cgen2%7Cdat%7Cacc%7Cacc2%7Cins%7Cloc%7Cloc2%7Cadnum)&sem-mod1=sem&sem-mod1=sem2&level1=0&level2=0&endyear=2019&parent2=0&parent1=0&min1=1&out=kwic&nodia=1&mode=main&p=\"\n",
    "rnc_query_page = 1\n",
    "rnc_query_page_offset = -1\n",
    "rnc_query_startyear = \"1900\"\n",
    "rnc_query_endyear = \"2019\"\n",
    "rnc_query_paged_template = \"http://processing.ruscorpora.ru/search.xml?sort=i_grtagging&lang=ru&lex1={lex}&startyear=\" + rnc_query_startyear + \"&text=lexgramm&max1=1&sem-mod2=sem&sem-mod2=sem2&gramm1=S,(nom%7Cvoc%7Cgen%7Cgen2%7Cdat%7Cacc%7Cacc2%7Cins%7Cloc%7Cloc2%7Cadnum)&sem-mod1=sem&sem-mod1=sem2&level1=0&level2=0&endyear=\" + rnc_query_endyear + \"&parent2=0&parent1=0&min1=1&out=kwic&nodia=1&mode=main&p={page}\"\n",
    "rnc_strip_characters = ' -—―,;:!?.\"“«»()[]/%<>'\n",
    "\n",
    "input_dir = \"input/\"\n",
    "output_dir = \"output/\"\n",
    "headers = [\"precontext\", \"preceding\", \"kwic\", \"following\", \"postcontext\", \"metadata\", \"date\"]\n",
    "header_text = {\n",
    "                \"precontext\": \"PRE_CONTEXT\",\n",
    "                \"preceding\": \"PRECEDING_WORD\",\n",
    "                \"kwic\": \"KWIC\",\n",
    "                \"following\": \"FOLLOWING_WORD\",\n",
    "                \"postcontext\": \"POST_CONTEXT\",\n",
    "                \"metadata\": \"METADATA\",\n",
    "                \"date\": \"DATE\"}\n",
    "\n",
    "rate_limit = 200  # milliseconds\n",
    "default_max_limit = 200000  # default limit for queries performed\n",
    "response_time = 0  # save last response time for backing off\n",
    "verbose = False  # verbose printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queues for visiting pages\n",
    "# pages to visit\n",
    "pages_queue = {}\n",
    "# pages queried\n",
    "pages_queried = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up verboseprint as a function if verbose printing is set\n",
    "verboseprint = print if verbose else lambda *a, **k: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnc_html(rnc_lex, rnc_page, rnc_set=0, use_new=False):\n",
    "    global response_time\n",
    "    filename = \"RNC_{lex}_{page}.html\".format(lex=rnc_lex, page=rnc_page)\n",
    "    filename_withset = \"RNC_{set}_{lex}_{page}.html\".format(set=rnc_set, lex=rnc_lex, page=rnc_page)\n",
    "\n",
    "    if rnc_lex not in pages_queue or pages_queue[rnc_lex] is None:\n",
    "        pages_queue[rnc_lex] = []\n",
    "    if rnc_lex not in pages_queried or pages_queried[rnc_lex] is None:\n",
    "        pages_queried[rnc_lex] = []\n",
    "\n",
    "    if not use_new:\n",
    "        try:\n",
    "            if os.path.isfile(input_dir + filename_withset):\n",
    "                f = open(input_dir + filename_withset)\n",
    "                used_filename = filename_withset\n",
    "            else:\n",
    "                f = open(input_dir + filename)\n",
    "                used_filename = filename\n",
    "        except OSError as err:\n",
    "            pass\n",
    "        else:\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            verboseprint(\"Using saved file \" + used_filename)\n",
    "            if rnc_lex in pages_queue and rnc_page in pages_queue[rnc_lex]:\n",
    "                pages_queue[rnc_lex].remove(rnc_page)\n",
    "            if rnc_lex in pages_queried and rnc_page not in pages_queried[rnc_lex]:\n",
    "                pages_queried[rnc_lex].append(int(rnc_page))\n",
    "\n",
    "            return text\n",
    "\n",
    "#   fetch new html data and write it to input_dir\n",
    "    if response_time > 0:\n",
    "        rate_ms = rate_limit / 1000\n",
    "        delay_time = rate_ms + 2*response_time\n",
    "        verboseprint(\"Waiting for \", str(delay_time), \"s; response time :\", str(response_time))\n",
    "        time.sleep(delay_time)\n",
    "    else:\n",
    "        verboseprint(\"Rate limit for \", str(rate_limit), \" milliseconds.\")\n",
    "        time.sleep(rate_limit / 1000)\n",
    "\n",
    "    verboseprint(\"Requesting query page for \" + rnc_lex + \" page \" + str(rnc_page))\n",
    "    t0 = time.time()\n",
    "    # get RNC html raw page using the visual page number, not the p= query value\n",
    "    rnc_html = requests.get(rnc_query_paged_template.format(lex=rnc_lex, page=rnc_page + rnc_query_page_offset)).text\n",
    "    response_time = time.time() - t0\n",
    "\n",
    "    if rnc_lex in pages_queried:\n",
    "        pages_queried[rnc_lex].append(rnc_page)\n",
    "    else:\n",
    "        pages_queried[rnc_lex] = [rnc_page]\n",
    "    verboseprint(\" \".join([\"added to pages queried:\", rnc_lex, str(rnc_page)]))\n",
    "\n",
    "    if rnc_lex in pages_queue and rnc_page in pages_queue[rnc_lex]:\n",
    "        pages_queue[rnc_lex].remove(rnc_page)\n",
    "        verboseprint(\" \".join([\"removed from page queue:\", rnc_lex, str(rnc_page)]))\n",
    "\n",
    "    try:\n",
    "        f = open(input_dir + filename, \"w\")\n",
    "        f.write(rnc_html)\n",
    "        f.close()\n",
    "    except OSError as err:\n",
    "        verboseprint(\"OS error: {0}\".format(err))\n",
    "        print(\"Could not save html file for \" + rnc_lex + \" page \" + str(rnc_page))\n",
    "\n",
    "    return rnc_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnc_text_table(rnc_html_raw):\n",
    "    rnc_html = bs4.BeautifulSoup(rnc_html_raw, \"lxml\")\n",
    "    rnc_tables = rnc_html.find_all(\"table\")\n",
    "\n",
    "    if rnc_tables is None or len(rnc_tables) is 0:\n",
    "        return \"\"\n",
    "\n",
    "    for table in rnc_tables:\n",
    "        if len(table.find_all(\"table\")) > 0:\n",
    "            # find the table that contains more tables, where the keyword in context results live\n",
    "            # assumption: there is only 1 table with nested tables.\n",
    "            # If this assumption is not true, the first nesting table will be returned\n",
    "            return table\n",
    "\n",
    "    return \"\"\n",
    "#     return rnc_tables ## returns all tables, which is not a good failure mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnc_rows(rnc_table, attr=headers):\n",
    "    #    find the content for each row\n",
    "    #    select sections by attributes in the rnc_row\n",
    "    if rnc_table is \"\":\n",
    "        rows = [{\"kwic\": \"RNC_NOT_FOUND\", }]\n",
    "        return rows\n",
    "\n",
    "    rnc_rows = rnc_table.find_all(\"tr\", recursive=False)\n",
    "    if rnc_rows is None or len(rnc_rows) == 0:\n",
    "        # if no <tr> elements, check for an <ol> element in rnc_table first\n",
    "        rnc_rows = rnc_table.find(\"ol\").find_all(\"tr\", recursive=False)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    if rnc_rows is not None and len(rnc_rows) > 0:\n",
    "        for row in rnc_rows:\n",
    "            selectdict = {}\n",
    "            tds = row.find_all(\"td\", recursive=False)\n",
    "            for i, td in enumerate(tds):\n",
    "                # counting of td elements is abnormal because of nested table structure and find_all(\"td\")\n",
    "                if i == 0 and (\"precontext\" in attr or \"preceding\" in attr):\n",
    "                    if \"precontext\" in attr and \"precontext\" not in selectdict:\n",
    "                        selectdict['precontext'] = unicodedata.normalize(\"NFKD\", td.text).strip()\n",
    "                    if \"preceding\" in attr and \"preceding\" not in selectdict:\n",
    "                        selectdict['preceding'] = unicodedata.normalize(\"NFKD\", td.text).strip(rnc_strip_characters).strip().split(\" \")[-1]\n",
    "                elif i == 1 and \"kwic\" in attr:\n",
    "                    selectdict['kwic'] = unicodedata.normalize(\"NFKD\", td.text).strip()\n",
    "                elif i == 2 and (\"postcontext\" in attr or \"following\" in attr):\n",
    "                    spans = td.find_all(\"span\")\n",
    "                    post_text = \" \".join([span.text for span in spans]).strip()\n",
    "                    if \"following\" in attr and \"following\" not in selectdict:\n",
    "                        selectdict['following'] = post_text.strip(rnc_strip_characters).strip().split(\" \")[0]\n",
    "                    if \"postcontext\" in attr and \"postcontext\" not in selectdict:\n",
    "                        selectdict['postcontext'] = post_text\n",
    "\n",
    "                if i == 2 and (\"metadata\" in attr or \"date\" in attr):\n",
    "                    # assume that <a> is <a class=\"b-kwic-expl\">\n",
    "                    verboseprint(td.a[\"msg\"])\n",
    "                    metadata = td.a[\"msg\"].strip()\n",
    "                    if len(metadata) < 1:\n",
    "                        metadata = 'RNC_UNSPECIFIED'\n",
    "                    if \"metadata\" in attr:\n",
    "                        selectdict['metadata'] = metadata\n",
    "                    if \"date\" in attr:\n",
    "                        date = re.search(r'\\d{4}', metadata)\n",
    "                        if date is None:\n",
    "                            date = 'RNC_UNSPECIFIED'\n",
    "                        else:\n",
    "                            date = date.group(0)\n",
    "                        selectdict['date'] = date\n",
    "            rows.append(selectdict)\n",
    "        return rows\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnc_next_pages(rnc_html_raw, rnc_lex, current_page=1):\n",
    "    rnc_soup = bs4.BeautifulSoup(rnc_html_raw, \"html.parser\")\n",
    "    rnc_pager = rnc_soup.find(\"p\", {\"class\": \"pager\"})\n",
    "    if rnc_pager is None:\n",
    "        return []\n",
    "    rnc_pages = rnc_pager.find_all(\"a\")\n",
    "    # only return next pages, not pages prior to current_page\n",
    "    pages = [int(page.text) for page in rnc_pages if (page.text.isdigit() and int(page.text) > current_page)]\n",
    "\n",
    "    if rnc_lex in pages_queue:\n",
    "        if pages_queue[rnc_lex] is None:\n",
    "            pages_queue[rnc_lex] = []\n",
    "        pages_queue[rnc_lex] = sorted(list(set(pages_queue[rnc_lex] + pages)))\n",
    "    else:\n",
    "        pages_queue[rnc_lex] = pages\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rnc_to_tsv(rnc_set, rnc_lex, rnc_rows, keys=headers):\n",
    "    filename = \"RNC_{set}_{lex}.tsv\".format(set=rnc_set, lex=rnc_lex)\n",
    "    try:\n",
    "        output_file = open(output_dir + filename, 'w')\n",
    "        dict_writer = csv.DictWriter(output_file, keys, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL, extrasaction=\"ignore\")\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(rnc_rows)\n",
    "        output_file.close()\n",
    "        print(\"Saved \" + str(len(rnc_rows)) + \" rows to \" + filename)\n",
    "        return True\n",
    "    except OSError as err:\n",
    "        verboseprint(\"OS error: {0}\".format(err))\n",
    "        print(\"Could not save tsv file for \" + rnc_lex + \" to \" + output_dir + filename)\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(word_list=word_pairs, resume_lex=\"\", sleep=rate_limit, max_limit=default_max_limit, verbose=False, **kwargs):\n",
    "    global rate_limit\n",
    "    if verbose:\n",
    "        verbose = True\n",
    "\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "\n",
    "    # clean up sleep and max_limit integers\n",
    "    if sleep <= 0:\n",
    "        rate_limit = 0\n",
    "    else:\n",
    "        rate_limit = sleep\n",
    "    if max_limit <= 0:\n",
    "        max_limit = default_max_limit\n",
    "\n",
    "    total_queries = 0\n",
    "    if resume_lex is \"\":\n",
    "        begin = True\n",
    "    else:\n",
    "        begin = False\n",
    "\n",
    "    for i_set, word_set in enumerate(word_list):\n",
    "        verboseprint(\"Processing set \", str(i_set), word_set)\n",
    "        verboseprint(\"Query max limit :\", str(max_limit), \"\\t total queries so far : \", str(total_queries))\n",
    "        if isinstance(word_set, str):\n",
    "            # if word_set is actually a word, make it into a 1-length list\n",
    "            word_set = [word_set]\n",
    "\n",
    "        for word in word_set:\n",
    "            if resume_lex == word:\n",
    "                verboseprint(\"Resume processing at\", resume_lex)\n",
    "                begin = True\n",
    "            if begin:\n",
    "                rows = []\n",
    "                pages_queue[word] = [rnc_query_page]\n",
    "                while len(pages_queue[word]) > 0 and (max_limit is 0 or total_queries <= max_limit):\n",
    "                    query_page = pages_queue[word].pop(0)\n",
    "                    current_html = get_rnc_html(word, query_page, rnc_set=i_set)\n",
    "                    get_rnc_next_pages(current_html, word, query_page)\n",
    "                    current_table = get_rnc_text_table(current_html)\n",
    "                    current_rows = get_rnc_rows(current_table)\n",
    "                    if len(current_rows) > 0:\n",
    "                        rows = rows + current_rows\n",
    "                    total_queries += 1\n",
    "                save_rnc_to_tsv(i_set, word, rows)\n",
    "            if total_queries >= max_limit:\n",
    "                begin = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_directories(input=\"\", output=\"\", **kwargs):\n",
    "    global input_dir\n",
    "    global output_dir\n",
    "\n",
    "    if input is not \"\" and os.path.isdir(input):\n",
    "        if not input.endswith(\"/\"):\n",
    "            input_dir = input + \"/\"\n",
    "        else:\n",
    "            input_dir = input\n",
    "    if output is not \"\" and os.path.isdir(output):\n",
    "        if not output.endswith(\"/\"):\n",
    "            output_dir = output + \"/\"\n",
    "        else:\n",
    "            output_dir = output\n",
    "    # try to make directories if they're not made yet\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    verboseprint(\"input directory:\\t\" + input_dir)\n",
    "    verboseprint(\"output directory:\\t\" + output_dir)\n",
    "    return {\n",
    "            \"input_dir\": input_dir,\n",
    "            \"output_dir\": output_dir\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_list(input_list=\"\", **kwargs):\n",
    "    word_list = []\n",
    "    if input_list is not \"\" and os.path.isfile(input_list):\n",
    "        try:\n",
    "            list_file = open(input_list, 'r', newline=\"\")\n",
    "            dialect = csv.Sniffer().sniff(list_file.read(1024))\n",
    "            list_file.seek(0)\n",
    "            dict_reader = csv.reader(list_file, dialect)  # , delimiter='\\t', quotechar='\"'\n",
    "            dict_list = list(dict_reader)\n",
    "            if len(dict_list) > 0:\n",
    "                for row in dict_list:\n",
    "                    while (\"\" in row):\n",
    "                        row.remove(\"\")\n",
    "                    if len(row):\n",
    "                        word_list.append(row)\n",
    "            else:\n",
    "                print(\"No words to query found in \" + input_list)\n",
    "            list_file.close()\n",
    "            verboseprint(\"Read \" + str(len(word_list)) + \" rows from \" + input_list)\n",
    "        except OSError as err:\n",
    "            verboseprint(\"OS error: {0}\".format(err))\n",
    "            print(\"Could not read tsv file from \" + filename)\n",
    "    elif input_list is not \"\":\n",
    "        string_list = input_list.splitlines()\n",
    "        if len(string_list) == 1:\n",
    "            # if string isn't split by lines, try splitting by \",\", then finding all sets of word characters [\\w]+\n",
    "            string_list = input_list.split(\",\")\n",
    "            if len(string_list) == 1:\n",
    "                word_list = re.findall(r\"[\\w]+\", string_list[0])\n",
    "                while (\"\" in word_list):\n",
    "                    word_list.remove(\"\")\n",
    "            else:\n",
    "                for words in string_list:\n",
    "                    words = re.findall(r\"[\\w]+\", words)\n",
    "                    while (\"\" in words):\n",
    "                        words.remove(\"\")\n",
    "                    word_list.append(words)\n",
    "        elif len(string_list) > 1:\n",
    "            for line in string_list:\n",
    "                words = re.findall(r\"[\\w]+\", line)\n",
    "                while (\"\" in words):\n",
    "                    words.remove(\"\")\n",
    "                word_list.append(words)\n",
    "\n",
    "    if len(word_list) is 0 and len(word_pairs) > 0:\n",
    "        word_list = word_pairs\n",
    "    elif len(word_list) is 0 and len(word_pairs) > 0:\n",
    "        print(\"No words to query.\")\n",
    "        return []\n",
    "\n",
    "    print(\"Word list to query:\")\n",
    "    print('\\t','\\n\\t'.join(['\\t'.join([str(word) for word in row]) for row in word_list]))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnc_html = get_rnc_html(word_pairs[-1][1], rnc_query_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseprint(rnc_query_paged_template.format(lex = word_pairs[0][0], page = rnc_query_page + rnc_query_page_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rnc_next_pages(test_rnc_html, word_pairs[0][0])\n",
    "# print(pages_queue)\n",
    "test_2_html = get_rnc_html(word_pairs[0][0], rnc_query_page+3)\n",
    "# print(pages_queried)\n",
    "# print(pages_queue)\n",
    "test_3_html = get_rnc_html(word_pairs[0][0], rnc_query_page+5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnc_soup = bs4.BeautifulSoup(test_rnc_html, 'html.parser')\n",
    "# print(test_rnc_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnc_table = get_rnc_text_table(test_rnc_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_rnc_table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_rnc_table.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnc_rows = get_rnc_rows(test_rnc_table, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 rows to RNC_0_тест.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_rnc_to_tsv(0, word_pairs[0][0], test_rnc_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main(sleep = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing resume_lex feature of main()\n",
    "# main(resume_lex=\"лимит\", sleep = 200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'питание': [], 'тест': [2, 3, 5, 7, 8, 9, 10, 11]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'питание': [1], 'тест': [4, 6]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_queried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
